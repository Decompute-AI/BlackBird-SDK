"""
Image generation routes module for Decompute Windows backend
Handles Flux model loading, image generation, and memory management
"""


from flask import Blueprint, request, jsonify, send_file
import os
import uuid
import json
import threading
import time
from datetime import datetime
import io
import base64
import logging
# Additional imports for image generation

from io import BytesIO
from PIL import Image, PngImagePlugin
import logging
import traceback
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# Import from core modules
from core.config import (
    BASE_UPLOAD_FOLDER, UPLOAD_FOLDER, HF_TOKEN
)
from content_filter import ContentFilter, add_watermark, add_metadata_to_image, dict_to_pnginfo
content_filter = ContentFilter()

# Create blueprint
image_bp = Blueprint('image', __name__)

# Global variables for image generation
flux_pipeline = None
model_loading_lock = threading.Lock()
is_loading_model = False


@image_bp.route('/images/<image_id>', methods=['GET'])
def get_image(image_id):
    """Retrieve a previously generated image by its ID"""
    output_dir = os.path.join(BASE_UPLOAD_FOLDER, 'output_images')
    file_path = os.path.join(output_dir, f"{image_id}.png")
    
    if os.path.exists(file_path):
        return send_file(file_path, mimetype='image/png')
    else:
        return jsonify({"error": "Image not found"}), 404

def get_flux_pipeline():
    """Get or initialize the Flux pipeline"""
    global flux_pipeline
    try:
        if flux_pipeline is None:
            print("Loading Flux model...")
            import torch
            from diffusers import FluxPipeline
            
            # Clear GPU memory before loading
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            flux_pipeline = FluxPipeline.from_pretrained(
                "black-forest-labs/FLUX.1-schnell", 
                torch_dtype=torch.bfloat16
            )
            flux_pipeline.to("cuda" if torch.cuda.is_available() else "cpu")
            
            # Enable memory efficient attention
            flux_pipeline.enable_model_cpu_offload()
            
            print("Flux model loaded successfully")
        
        return flux_pipeline
    except Exception as e:
        print(f"Error loading Flux model: {str(e)}")
        raise

def clear_gpu_memory():
    """Clear GPU memory and perform cleanup"""
    try:
        import torch
        import gc
        
        # Force garbage collection
        gc.collect()
        
        # Clear PyTorch CUDA cache if available
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            print("GPU memory cleared successfully")
        else:
            print("No CUDA available, cleared system memory")
            
    except Exception as e:
        print(f"Error clearing GPU memory: {str(e)}")

def is_content_appropriate(prompt):
    """Basic content filtering for image generation prompts"""
    inappropriate_keywords = [
        'nude', 'naked', 'nsfw', 'explicit', 'sexual', 'porn', 'erotic',
        'violence', 'gore', 'blood', 'weapon', 'drug', 'hate', 'racist'
    ]
    
    prompt_lower = prompt.lower()
    for keyword in inappropriate_keywords:
        if keyword in prompt_lower:
            return False, f"Content not allowed: '{keyword}' detected"
    
    return True, "Content approved"

def add_watermark(image, text="Generated by Decompute"):
    """Add watermark to generated image"""
    try:
        from PIL import ImageDraw, ImageFont
        
        # Create a copy of the image
        watermarked = image.copy()
        draw = ImageDraw.Draw(watermarked)
        
        # Try to use a system font, fallback to default
        try:
            font = ImageFont.truetype("arial.ttf", 20)
        except:
            font = ImageFont.load_default()
        
        # Get image dimensions
        width, height = watermarked.size
        
        # Calculate text position (bottom right)
        bbox = draw.textbbox((0, 0), text, font=font)
        text_width = bbox[2] - bbox[0]
        text_height = bbox[3] - bbox[1]
        
        x = width - text_width - 10
        y = height - text_height - 10
        
        # Add semi-transparent background
        draw.rectangle([x-5, y-5, x+text_width+5, y+text_height+5], 
                      fill=(0, 0, 0, 128))
        
        # Add text
        draw.text((x, y), text, font=font, fill=(255, 255, 255, 255))
        
        return watermarked
    except Exception as e:
        print(f"Error adding watermark: {str(e)}")
        return image


def enhance_prompt(prompt, level="light"):
    """Prompt enhancement with different levels for speed vs quality"""
    if level == "none":
        return prompt
    elif level == "light":
        # Minimal enhancement
        if "detailed" not in prompt.lower():
            prompt += ", detailed"
    elif level == "medium":
        # Moderate enhancement
        if "high quality" not in prompt.lower():
            prompt += ", high quality"
        if "detailed" not in prompt.lower():
            prompt += ", detailed"
    elif level == "heavy":
        # Full enhancement (slower)
        if "high quality" not in prompt.lower():
            prompt += ", high quality, detailed"
        if "4k" not in prompt.lower() and "8k" not in prompt.lower():
            prompt += ", 4k"
    return prompt


def save_png_with_chunks(image, file_path, metadata=None):
    """Save a PNG image with metadata chunks preserved"""
    if metadata:
        image = add_metadata_to_image(image, metadata, keep_format="PNG")
        chunks_dict = getattr(image, "text", None) or image.info
        pnginfo_obj = dict_to_pnginfo(chunks_dict)
        image.save(file_path, format="PNG", pnginfo=pnginfo_obj)
    else:
        image.save(file_path, format="PNG")
    
    return file_path



@image_bp.route('/generate', methods=['POST'])
def generate_image():
    """
    Generate an image using Flux for Windows
    Expected JSON payload: {
        "prompt": "a photo of an astronaut riding a horse on mars",
        "steps": 4,  # optional, default 4 for schnell
        "save": true,  # optional
        "seed": 42,  # optional
        "width": 1024,  # optional
        "height": 1024,  # optional
        "watermark": true,  # optional
        "include_metadata": true,  # optional
        "user_email": "user@example.com"  # optional
        "filter_level": "moderate"  # optional: "strict", "moderate", "minimal"
        "enhance_prompt": false  # optional: whether to enhance the prompt
    }
    """
    global flux_pipeline
    
    # Get request data
    data = request.json
    
    # Extract parameters from request
    prompt = data.get('prompt')
    if not prompt:
        return jsonify({"error": "Prompt is required"}), 400
    
    # Content filtering using the proper ContentFilter class
    filter_level = data.get("filter_level", "moderate")
    is_appropriate = content_filter.is_appropriate(prompt, filter_level)
    if not is_appropriate:
        return jsonify({"error": "Inappropriate content detected"}), 400
    
    # Enhance the prompt only if requested (default: False for speed)
    enhance_prompt_flag = data.get('enhance_prompt', False)
    enhance_level = data.get('enhance_level', 'light')  # none, light, medium, heavy
    if enhance_prompt_flag:
        prompt = enhance_prompt(prompt, level=enhance_level)
    
    # Extract optional parameters
    steps = int(data.get('steps', 4))  # Default to 4 steps for schnell
    save_to_disk = data.get('save', False)
    watermark_enabled = bool(data.get("watermark", True))
    include_metadata = bool(data.get("include_metadata", True))
    user_email = data.get("user_email", "sample@gmail.com")
    seed = data.get('seed', 42)
    width = int(data.get('width', 1024))
    height = int(data.get('height', 1024))
    
    # Load model if not already done
    if flux_pipeline is None:
        success = load_flux_model()
        if not success:
            return jsonify({"error": "Failed to load Flux model"}), 500
    
    try:
        # Log the request
        logger.info(f"Generating image with prompt: '{prompt}', steps: {steps}, seed: {seed}")
        
        # Set up generator for reproducible results
        import torch
        generator = torch.Generator().manual_seed(seed) if seed else None
        
        # Generate the image using your provided code structure
        output = flux_pipeline(
            [prompt],  # prompts as list for batch processing
            num_inference_steps=steps,
            guidance_scale=0.0,  # Schnell doesn't use guidance
            height=height,
            width=width,
            generator=generator,
        )
        
        image = output.images[0]
        
        # Generate a unique ID for this image
        image_id = str(uuid.uuid4())
        
        # Add watermark if enabled using the proper function from content_filter.py
        if watermark_enabled:
            watermark_text = "Generated locally by Decompute"
            watermark_position = "bottom-right"
            watermark_opacity = 0.8
            image = add_watermark(
                image, 
                text=watermark_text, 
                opacity=watermark_opacity, 
                position=watermark_position
            )
        
        # Add metadata if enabled using the proper function from content_filter.py
        if include_metadata:
            metadata = {
                "prompt": prompt,
                "steps": steps,
                "seed": seed,
                "width": width,
                "height": height,
                "model": "FLUX.1-schnell",
                "generator": "Decompute Local Image Generator (Windows)",
                "generation_date": datetime.now().isoformat(),
                "image_id": image_id,
                "user_email": user_email
            }
            image = add_metadata_to_image(image, metadata, keep_format="PNG")
        
        # Save to disk if requested
        if save_to_disk:
            output_dir = os.path.join(BASE_UPLOAD_FOLDER, 'output_images')
            os.makedirs(output_dir, exist_ok=True)
            file_path = os.path.join(output_dir, f"{image_id}.png")
            save_png_with_chunks(image, file_path, metadata)
            logger.info("Image saved to %s", file_path)
            return jsonify({
                "success": True, 
                "message": "Image generated and saved", 
                "image_id": image_id, 
                "file_path": file_path
            })
        else:
            # Return image directly
            img_io = BytesIO()
            image.save(img_io, 'PNG')
            img_io.seek(0)
            return send_file(img_io, mimetype='image/png')
            
    except Exception as e:
        logger.error(f"Error during image generation: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({"error": str(e)}), 500


@image_bp.route('/api/image/model-status', methods=['GET'])
def get_model_status():
    """Get the current status of the Flux model"""
    try:
        global flux_pipeline, is_loading_model
        
        status = {
            'model_loaded': flux_pipeline is not None,
            'loading': is_loading_model,
            'model_name': 'FLUX.1-schnell' if flux_pipeline is not None else None
        }
        
        # Add GPU memory info if available
        try:
            import torch
            if torch.cuda.is_available():
                status['gpu_available'] = True
                status['gpu_memory_allocated'] = torch.cuda.memory_allocated()
                status['gpu_memory_cached'] = torch.cuda.memory_reserved()
                status['gpu_memory_total'] = torch.cuda.get_device_properties(0).total_memory
            else:
                status['gpu_available'] = False
        except Exception:
            status['gpu_available'] = False
        
        return jsonify(status)
        
    except Exception as e:
        print(f"Error getting model status: {str(e)}")
        return jsonify({'error': str(e)}), 500

@image_bp.route('/api/image/load-model', methods=['POST'])
def load_flux_model():
    """Load the Flux model for Windows"""
    global flux_pipeline
    
    if flux_pipeline is not None:
        return True
    
    try:
        from diffusers import FluxPipeline
        import torch
        import gc
        
        DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
        DTYPE = torch.bfloat16 if DEVICE == "cuda" else torch.float32
        CKPT_ID = "black-forest-labs/FLUX.1-schnell"
        
        logger.info(f"Loading Flux model on {DEVICE} with dtype {DTYPE}")
        
        # Clear GPU memory before loading Flux model for optimal performance
        if DEVICE == "cuda":
            torch.cuda.empty_cache()
            gc.collect()
            logger.info("Cleared GPU cache before loading Flux model")
        
        flux_pipeline = FluxPipeline.from_pretrained(
            CKPT_ID,
            torch_dtype=DTYPE,
            token=HF_TOKEN,
        )
        
        # Memory optimizations - use the same pattern as working standalone script
        if DEVICE == "cuda":
            flux_pipeline.vae.enable_tiling()
            flux_pipeline.vae.enable_slicing()
            flux_pipeline.enable_sequential_cpu_offload()
            # Don't call .to(DEVICE) when using sequential_cpu_offload
        else:
            # Only move to CPU if we're not using CUDA
            flux_pipeline.to(DEVICE)
        
        logger.info("Flux model loaded successfully")
        return True
        
    except Exception as e:
        logger.error(f"Failed to load Flux model: {str(e)}")
        return False



def dict_to_pnginfo(chunks):
    """Convert a dictionary to PngInfo object"""
    pi = PngImagePlugin.PngInfo()
    for k, v in chunks.items():
        pi.add_text(k, str(v))  # Make sure values are strings
    return pi

@image_bp.route('/api/image/model-status', methods=['GET'])
def get_image_model_status():
    """Check if the image generation model is loaded"""
    global flux_pipeline
    return jsonify({
        "loaded": flux_pipeline is not None,
        "model": "FLUX.1-schnell" if flux_pipeline is not None else None
    })

@image_bp.route('/api/image/load-model', methods=['POST'])
def load_image_model():
    """Manually load the image generation model"""
    success = load_flux_model()
    if success:
        return jsonify({"success": True, "message": "Flux model loaded successfully"})
    else:
        return jsonify({"success": False, "message": "Failed to load Flux model"}), 500

@image_bp.route('/api/image/unload-model', methods=['POST'])
def unload_image_model():
    """Unload the image generation model to free memory"""
    global flux_pipeline
    
    try:
        if flux_pipeline is not None:
            del flux_pipeline
            flux_pipeline = None
            
            # Clear GPU memory if available
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # Force garbage collection
            import gc
            gc.collect()
            
            logger.info("Flux model unloaded successfully")
            return jsonify({"success": True, "message": "Image model unloaded successfully"})
        else:
            return jsonify({"success": True, "message": "No model was loaded"})
            
    except Exception as e:
        logger.error(f"Error unloading model: {str(e)}")
        return jsonify({"success": False, "message": f"Error unloading model: {str(e)}"}), 500

# Ensure output images directory exists
output_images_dir = os.path.join(BASE_UPLOAD_FOLDER, 'output_images')
os.makedirs(output_images_dir, exist_ok=True)


@image_bp.route('/api/image/generate-batch', methods=['POST'])
def generate_batch_images():
    """Generate multiple images from a list of prompts"""
    try:
        data = request.get_json()
        prompts = data.get('prompts', [])
        
        if not prompts:
            return jsonify({'error': 'Prompts list is required'}), 400
        
        if len(prompts) > 5:  # Limit batch size
            return jsonify({'error': 'Maximum 5 prompts allowed per batch'}), 400
        
        results = []
        errors = []
        
        for i, prompt in enumerate(prompts):
            try:
                # Content filtering
                is_appropriate, message = is_content_appropriate(prompt)
                if not is_appropriate:
                    errors.append(f"Prompt {i+1}: {message}")
                    continue
                
                # Generate image ID
                image_id = str(uuid.uuid4())
                
                # Get pipeline
                pipeline = get_flux_pipeline()
                
                # Generate image
                result = pipeline(
                    prompt,
                    guidance_scale=0.0,
                    num_inference_steps=4,
                    max_sequence_length=256,
                    generator=None
                )
                
                generated_image = result.images[0]
                watermarked_image = add_watermark(generated_image)
                
                # Save image
                images_dir = os.path.join(BASE_UPLOAD_FOLDER, 'generated_images')
                os.makedirs(images_dir, exist_ok=True)
                
                image_path = os.path.join(images_dir, f"{image_id}.png")
                watermarked_image.save(image_path, "PNG")
                
                # Save metadata
                metadata = {
                    'id': image_id,
                    'prompt': prompt,
                    'timestamp': datetime.now().isoformat(),
                    'model': 'FLUX.1-schnell',
                    'batch_index': i
                }
                
                metadata_path = os.path.join(images_dir, f"{image_id}_metadata.json")
                with open(metadata_path, 'w') as f:
                    json.dump(metadata, f)
                
                results.append({
                    'image_id': image_id,
                    'prompt': prompt,
                    'success': True
                })
                
            except Exception as e:
                errors.append(f"Prompt {i+1}: {str(e)}")
        
        return jsonify({
            'success': len(results) > 0,
            'results': results,
            'errors': errors,
            'total_generated': len(results),
            'total_errors': len(errors)
        })
        
    except Exception as e:
        print(f"Error in batch generation: {str(e)}")
        return jsonify({'error': str(e)}), 500

@image_bp.route('/api/image/list', methods=['GET'])
def list_generated_images():
    """List all generated images with metadata"""
    try:
        images_dir = os.path.join(BASE_UPLOAD_FOLDER, 'generated_images')
        
        if not os.path.exists(images_dir):
            return jsonify({'images': []})
        
        images = []
        
        for filename in os.listdir(images_dir):
            if filename.endswith('_metadata.json'):
                metadata_path = os.path.join(images_dir, filename)
                image_id = filename.replace('_metadata.json', '')
                image_path = os.path.join(images_dir, f"{image_id}.png")
                
                if os.path.exists(image_path):
                    try:
                        with open(metadata_path, 'r') as f:
                            metadata = json.load(f)
                        
                        # Add file size
                        file_size = os.path.getsize(image_path)
                        metadata['file_size'] = file_size
                        
                        images.append(metadata)
                    except Exception as e:
                        print(f"Error reading metadata for {image_id}: {str(e)}")
        
        # Sort by timestamp (newest first)
        images.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
        
        return jsonify({
            'images': images,
            'total_count': len(images)
        })
        
    except Exception as e:
        print(f"Error listing images: {str(e)}")
        return jsonify({'error': str(e)}), 500



@image_bp.route('/api/image/optimize-memory', methods=['POST'])
def optimize_memory_for_image():
    """Clear other models from GPU to optimize for image generation"""
    global rag_chat, _CURRENT_MODEL, _CURRENT_TOKENIZER
    
    try:
        import torch
        import gc
        
        # Clear RAG chat models
        if rag_chat is not None:
            del rag_chat
            rag_chat = None
            logger.info("Cleared RAG chat model")
        
        # Clear default chat models
        if '_CURRENT_MODEL' in globals() and _CURRENT_MODEL is not None:
            del _CURRENT_MODEL
            _CURRENT_MODEL = None
            logger.info("Cleared default chat model")
            
        if '_CURRENT_TOKENIZER' in globals() and _CURRENT_TOKENIZER is not None:
            del _CURRENT_TOKENIZER  
            _CURRENT_TOKENIZER = None
            logger.info("Cleared default chat tokenizer")
        
        # Force garbage collection and clear GPU cache
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
        logger.info("Memory optimization complete")
        return jsonify({
            "success": True, 
            "message": "Memory optimized for image generation"
        })
        
    except Exception as e:
        logger.error(f"Error optimizing memory: {str(e)}")
        return jsonify({
            "success": False, 
            "message": f"Error optimizing memory: {str(e)}"
        }), 500
    
